{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import os\n",
    "import shutil\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_EMBEDDING_MODEL_PATH = \"Salesforce/SFR-Embedding-Mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = './markdown_pages'\n",
    "CHROMADB_FILEPATH = './chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(SOURCE_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMADB_FILEPATH):\n",
    "        shutil.rmtree(CHROMADB_FILEPATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, HuggingFaceEmbeddings(), persist_directory=CHROMADB_FILEPATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMADB_FILEPATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HuggingFaceEmbeddings.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m      2\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      3\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      4\u001b[0m     length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m,\n\u001b[1;32m      5\u001b[0m     add_start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(load_documents())\n\u001b[0;32m----> 9\u001b[0m \u001b[43msave_to_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m, in \u001b[0;36msave_to_chroma\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     33\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(CHROMADB_FILEPATH)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create a new DB from the documents.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m---> 37\u001b[0m     chunks, \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHF_EMBEDDING_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m, persist_directory\u001b[38;5;241m=\u001b[39mCHROMADB_FILEPATH\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m db\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHROMADB_FILEPATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: HuggingFaceEmbeddings.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(load_documents())\n",
    "save_to_chroma(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
